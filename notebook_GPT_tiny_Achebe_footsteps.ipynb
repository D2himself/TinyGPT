{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epYC9gn1cEPN"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "\n",
        "# url = \"https://archive.org/stream/AchebeChinuaThingsFallApart/Achebe%20Chinua%20-%20Things%20Fall%20Apart_djvu.txt\"\n",
        "# text = requests.get(url).text\n",
        "\n",
        "# # Basic cleaning\n",
        "# start = text.find(\"CHAPTER ONE\")\n",
        "# end = text.rfind(\"The End\") if \"The End\" in text else len(text)\n",
        "# novel_text = text[start:end]\n",
        "\n",
        "# with open(\"things_fall_apart.txt\", \"w\") as f:\n",
        "#     f.write(novel_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('things_fall_apart.txt', 'r', encoding='utf-8') as f:\n",
        "#   text = f.read()"
      ],
      "metadata": {
        "id": "KtMZ1AjXdKPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')#force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoAwuNJb3j6Q",
        "outputId": "5cc7da8c-692e-486d-ee09-1e3f752b66d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "files = glob.glob('/content/drive/MyDrive/achebe_texts/*.txt')\n",
        "\n",
        "\n",
        "all_texts = []\n",
        "for f in files:\n",
        "    with open(f, 'r', encoding='utf-8') as infile:\n",
        "        text = infile.read()\n",
        "\n",
        "        # basic cleaning\n",
        "        text = text.replace('\\n', ' ')  # remove line breaks\n",
        "        text = \" \".join(text.split())   # collapse multiple spaces\n",
        "        all_texts.append(f\"<|book_start|>\\n{text}\\n<|book_end|>\\n\")\n",
        "\n",
        "# save combined dataset\n",
        "with open(\"achebe_corpus.txt\", \"w\", encoding=\"utf-8\") as outfile:\n",
        "    outfile.write(\"\\n\".join(all_texts))"
      ],
      "metadata": {
        "id": "XGI36V7l4r2b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('achebe_corpus.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "q-4gcnWCcUPE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoCkXyB_c4LA",
        "outputId": "c64c097a-5268-41fb-8366-abf22fd47f96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2027360"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.makedirs(('bpe_tokenizer'))\n",
        "\n",
        "# from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# # train tokenizer on text\n",
        "# tokenizer.train(files=\"achebe_corpus.txt\", vocab_size=5000, min_frequency=2, special_tokens=[\n",
        "#     \"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\",\n",
        "# ])\n",
        "\n",
        "# # save vocab + merges\n",
        "# tokenizer.save_model(\"bpe_tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibfUTk2gtnhI",
        "outputId": "6df52fcd-a221-4601-e2e0-5cfac779971e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bpe_tokenizer/vocab.json', 'bpe_tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 512\n",
        "max_iters = 10000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# with open('things_fall_apart.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# total training steps = max_iters\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for first warmup_iters steps\n",
        "    warmup_iters = 1000\n",
        "    # 2) cosine decay down to min learning rate\n",
        "    lr_min = 1e-5\n",
        "    lr_max = 3e-4\n",
        "    # 3) calculate\n",
        "    if it < warmup_iters:\n",
        "        return lr_max * it / warmup_iters\n",
        "    if it > max_iters:\n",
        "        return lr_min\n",
        "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
        "    decay_ratio = min(1, decay_ratio)\n",
        "    # cosine decay\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return lr_min + coeff * (lr_max - lr_min)\n",
        "\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        lr = get_lr(iter)\n",
        "        print(f\"step {iter}: lr = {lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # generate a sample\n",
        "        prompt = \"Okonkwo was \"\n",
        "        x = torch.tensor(encode(prompt), dtype=torch.long, device=device)[None, ...]\n",
        "        out = m.generate(x, max_new_tokens=100)\n",
        "        print(decode(out[0].tolist()).replace(\"\\n\", \" \"))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update lr with scheduler\n",
        "    lr = get_lr(iter)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe_KBhIRiTY_",
        "outputId": "5b56b91b-660b-4890-f58b-d50b6f487d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.824303 M parameters\n",
            "step 0: train loss 4.8680, val loss 4.8665\n",
            "step 500: train loss 2.1493, val loss 2.1073\n",
            "step 1000: train loss 1.6581, val loss 1.6232\n",
            "step 1500: train loss 1.4702, val loss 1.4494\n",
            "step 2000: train loss 1.3593, val loss 1.3518\n",
            "step 2500: train loss 1.2869, val loss 1.2937\n",
            "step 3000: train loss 1.2394, val loss 1.2591\n",
            "step 3500: train loss 1.1973, val loss 1.2386\n",
            "step 4000: train loss 1.1674, val loss 1.2180\n",
            "step 4500: train loss 1.1429, val loss 1.2044\n",
            "step 4999: train loss 1.1186, val loss 1.1925\n",
            "Okonkwo was cleaning officering into Eduka’s important of, too hear the man hands. But there were quickly two crows of station that would not see the myself for white orubs my shinious most opinion action, the ga\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Okonkwo was preparing for the feast when \"\n",
        "x = torch.tensor(encode(prompt), dtype=torch.long, device=device)[None, ...]\n",
        "out = m.generate(x, max_new_tokens=1000)\n",
        "print(decode(out[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZpn3x07DuCR",
        "outputId": "50e26b7e-890c-4017-a4d1-0adce4669050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okonkwo was preparing for the feast when they had yet since the end yet. Okonkwo received the money’s mother he had come for spiciout—the ear and spat into its finger. When he finally left his Nkechokwu ignored his flute, had treachering highe ripe in funeir intervolunt that he took his interpretexity. Ogbuildine, Ugoye, Ugoye, was Many humanueduna. But some thing he had indeed history at the return which had forced on a breast but lowly remained for his friend in rag whether for his evening that day we were the kind of the primindships. It was the fact of the boys of the Omagazine darted! You shook our doright, why, then your brew that was daily astraightening. And I said to indioguaning the English stories and the world is now perfect that.' As it was heard wanting their fells and genomed people gets away. After anyone woulding finds that statement were or senior naturally. 'You might believary coool friends,' says those who had been costep—was impiring in split conceration. That would the secondary coup. As I took my witny\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5392eee3"
      },
      "source": [
        "**Notebook Summary**\n",
        "\n",
        "This notebook demonstrates the process of training a character-level language model on a corpus of Chinua Achebe's novels.\n",
        "\n",
        "1.  **Data Acquisition and Preparation:**\n",
        "    *   Initially, an attempt was made to download \"Things Fall Apart\" from archive.org, clean it, and save it to a file (`epYC9gn1cEPN`). This step was commented out.\n",
        "    *   The commented-out code also included reading the saved text file (`KtMZ1AjXdKPE`).\n",
        "    *   Google Drive was mounted to access local files (`UoAwuNJb3j6Q`).\n",
        "    *   Text files from a specified Google Drive folder (`/content/drive/MyDrive/achebe_texts/`) were read, cleaned (removing line breaks and collapsing spaces), and combined into a single file named `achebe.txt`. Each book was marked with `<|book_start|>` and `<|book_end|>` tokens (`XGI36V7l4r2b`).\n",
        "    *   The combined text from `achebe.txt` was read into a variable (`q-4gcnWCcUPE`).\n",
        "    *   The first 1000 characters of the combined text were displayed to verify the content (`XoCkXyB_c4LA`).\n",
        "\n",
        "2.  **Tokenization (Attempted):**\n",
        "    *   Code to train a Byte Pair Encoding (BPE) tokenizer was included but commented out. This step would have created a tokenizer based on the combined text (`ibfUTk2gtnhI`).\n",
        "\n",
        "3.  **Language Model Training:**\n",
        "    *   A character-level Bigram Language Model (a simple transformer model) was defined using PyTorch (`Xe_KBhIRiTY_`).\n",
        "    *   Hyperparameters for the model were set (batch size, block size, learning rate, etc.).\n",
        "    *   A character vocabulary and mapping between characters and integers were created.\n",
        "    *   The combined text data was split into training and validation sets.\n",
        "    *   Functions were defined to get data batches and estimate the loss.\n",
        "    *   The transformer model architecture was defined, including `Head` (self-attention head), `MultiHeadAttention`, `FeedFoward`, and `Block`.\n",
        "    *   The `BigramLanguageModel` class was defined, incorporating token and position embeddings, transformer blocks, and a linear output layer.\n",
        "    *   The model was initialized and moved to the appropriate device (GPU if available, otherwise CPU). The number of model parameters was printed.\n",
        "    *   An AdamW optimizer was created.\n",
        "    *   The model was trained for a specified number of iterations, with loss reported periodically on both training and validation sets.\n",
        "\n",
        "4.  **Text Generation:**\n",
        "    *   After training, text was generated from the trained model using a prompt \"Okonkwo was \" (`Xe_KBhIRiTY_`).\n",
        "    *   Another text generation was performed with the prompt \"Okonkwo was preparing for the feast when \" (`wZpn3x07DuCR`).\n",
        "    *   The generated text from the second prompt was stored in a variable named `generation` (`1sNj5DEw4O84`).\n",
        "\n",
        "This notebook successfully demonstrates the process of loading, preparing, and using a custom transformer model for character-level text generation."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9wFZR8klWCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}